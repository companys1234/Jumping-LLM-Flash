{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/companys1234/Jumping_LLM_Flash.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQt6FnWRgvyw",
        "outputId": "47370991-88e0-41ee-f09a-8dedc97f9331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Jumping_LLM_Flash'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 147 (delta 42), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (147/147), 58.17 KiB | 416.00 KiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Jumping_LLM_Flash.scr.preprocessing import SimpleTokenizer\n",
        "from Jumping_LLM_Flash.scr.architecture import *\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "vMhNwYB6hePG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHgaAhlZcbEW",
        "outputId": "b4d03fe9-d3ba-4e92-8e1b-c32db5b4b5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-01 15:56:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  6.27MB/s    in 0.2s    \n",
            "\n",
            "2026-01-01 15:56:39 (6.27 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "KYQTjUtrdUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer(text)"
      ],
      "metadata": {
        "id": "ogu6cr4mdXas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, block_size, batch_size, device):\n",
        "    \"\"\"\n",
        "    data: torch.Tensor (1D)\n",
        "    block_size: int — длина контекста\n",
        "    \"\"\"\n",
        "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,), device=device)\n",
        "\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])  # (B, block_size)\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])  # (B, block_size)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "k7-tkLUIGEsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "mwxVIcdsPpW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 69\n",
        "class llama2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.rmsnorm = RMSNorm(128)\n",
        "    self.rope = RoPE\n",
        "    self.embedding = nn.Embedding(vocab_size, 128)\n",
        "    #self.swiglu = SwigLU\n",
        "    self.GMQA = GMQA_with_KV(128,4,1)\n",
        "    self.rmsnorm2 = RMSNorm(128*2)\n",
        "    self.ff = Feed_Forward_Network(128 * 2, 200, SwigLU())\n",
        "    self.rms3 = RMSNorm(128 * 3)  # после второй конкатенации\n",
        "    self.out_proj = nn.Linear(128 * 3, vocab_size)\n",
        "  def forward(self,x):\n",
        "        x= self.embedding(x)\n",
        "        gmqa_out, attn_weights, present_kv = self.GMQA(x,True)\n",
        "\n",
        "        concat1 = torch.cat([x, gmqa_out], dim=-1)\n",
        "        concat1_norm = self.rmsnorm2(concat1)\n",
        "        ff_out = self.ff(concat1_norm)\n",
        "\n",
        "        concat2 = torch.cat([gmqa_out, ff_out], dim=-1)\n",
        "        concat2_norm = self.rms3(concat2)\n",
        "\n",
        "        logits = self.out_proj(concat2_norm)\n",
        "        return logits, present_kv\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "        #past_kv = None\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.forward(idx)[0]\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "J3QJqzPqeF7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, criterion, device, epochs=42, batch_size=8, block_size=8):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for _ in range(len(data) // batch_size):\n",
        "            ix = torch.randint(0, len(data) - block_size - 1, (batch_size,), device=device)\n",
        "            x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "            y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / (len(data) // batch_size)\n",
        "        print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Z-qLv2a-HLYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()\n",
        "tokens = torch.tensor(tokenizer.encode(text[:10000]), dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13-aqg-BHQzp",
        "outputId": "3bafed16-ba20-4431-b8a2-60365ae14b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3419825771.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tokens = torch.tensor(tokenizer.encode(text[:6000]), dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# создаём модель\n",
        "model = llama2()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(\n",
        "    model=model,\n",
        "    data=tokens,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device = 'cpu'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LttV9DOQHdwE",
        "outputId": "3270a875-40c5-49b6-8aef-da484c13e168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=1.7163\n",
            "Epoch 2: loss=1.3028\n",
            "Epoch 3: loss=1.1708\n",
            "Epoch 4: loss=1.0772\n",
            "Epoch 5: loss=1.0060\n",
            "Epoch 6: loss=0.9518\n",
            "Epoch 7: loss=0.8939\n",
            "Epoch 8: loss=0.8525\n",
            "Epoch 9: loss=0.7981\n",
            "Epoch 10: loss=0.7669\n",
            "Epoch 11: loss=0.7367\n",
            "Epoch 12: loss=0.7133\n",
            "Epoch 13: loss=0.6882\n",
            "Epoch 14: loss=0.6613\n",
            "Epoch 15: loss=0.6305\n",
            "Epoch 16: loss=0.6302\n",
            "Epoch 17: loss=0.6142\n",
            "Epoch 18: loss=0.5985\n",
            "Epoch 19: loss=0.5756\n",
            "Epoch 20: loss=0.5706\n",
            "Epoch 21: loss=0.5637\n",
            "Epoch 22: loss=0.5432\n",
            "Epoch 23: loss=0.5414\n",
            "Epoch 24: loss=0.5286\n",
            "Epoch 25: loss=0.5187\n",
            "Epoch 26: loss=0.5219\n",
            "Epoch 27: loss=0.5072\n",
            "Epoch 28: loss=0.4984\n",
            "Epoch 29: loss=0.4870\n",
            "Epoch 30: loss=0.4794\n",
            "Epoch 31: loss=0.4830\n",
            "Epoch 32: loss=0.4786\n",
            "Epoch 33: loss=0.4717\n",
            "Epoch 34: loss=0.4592\n",
            "Epoch 35: loss=0.4571\n",
            "Epoch 36: loss=0.4488\n",
            "Epoch 37: loss=0.4544\n",
            "Epoch 38: loss=0.4405\n",
            "Epoch 39: loss=0.4375\n",
            "Epoch 40: loss=0.4359\n",
            "Epoch 41: loss=0.4357\n",
            "Epoch 42: loss=0.4297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, tokenizer, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "A7RgrxRV-BV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?:;'\\n-&3$#@э!?*\")\n",
        "\n",
        "vocab_size = 69\n",
        "\n",
        "prompt = \"First Citizen: We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they\"\n",
        "\n",
        "response = chat(model, tokenizer, prompt, device='cpu', max_new_tokens=50)\n",
        "print('response:',response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP79wDnW-D98",
        "outputId": "99f751c4-4619-4555-f118-830efe992454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response: First Citizen: We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if theyouce coursthe corithe corthe couche coucouche coun\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2681672045.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device)\n"
          ]
        }
      ]
    }
  ]
}