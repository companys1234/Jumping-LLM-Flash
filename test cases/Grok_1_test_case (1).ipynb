{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTdnnqVfGTm",
        "outputId": "35b04ae6-a333-4ea8-c35c-dc883a74632c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Jumping_LLM_Flash'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 304 (delta 52), reused 3 (delta 3), pack-reused 201 (from 1)\u001b[K\n",
            "Receiving objects: 100% (304/304), 120.02 KiB | 12.00 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!git clone https://github.com/companys1234/Jumping_LLM_Flash.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from Jumping_LLM_Flash.scr.preprocessing import BPE\n",
        "from Jumping_LLM_Flash.scr.architecture import *\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "6Xip33Oqjy7L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "NU7ctwFSj06p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6252a5ff-d314-48e0-9a90-7f63ba36366a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-28 09:57:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2026-01-28 09:57:44 (120 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "_b8o4frOj4SS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for tex in text:\n",
        "    words.extend(tex.split())"
      ],
      "metadata": {
        "id": "KKSMEk15g4AD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BPE(100)\n",
        "tokenizer.fit(words)\n"
      ],
      "metadata": {
        "id": "1Ksjgywwj4vD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, block_size, batch_size, device):\n",
        "    \"\"\"\n",
        "    data: torch.Tensor (1D)\n",
        "    block_size: int — длина контекста\n",
        "    \"\"\"\n",
        "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,), device=device)\n",
        "\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])  # (B, block_size)\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])  # (B, block_size)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "tT2_QEq1j6Ki"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Grok1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.num_experts = 8\n",
        "        self.embedding = nn.Embedding(260, 100)\n",
        "\n",
        "        self.expert = nn.ModuleList([nn.Sequential(\n",
        "            GQA(d_model=100, num_heads=4, num_kv_heads=None, pos_enc=RoPE),\n",
        "            GQA(d_model=100, num_heads=4, num_kv_heads=None, pos_enc=RoPE),\n",
        "            GQA(d_model=100, num_heads=4, num_kv_heads=None, pos_enc=RoPE)\n",
        "        ) for _ in range(self.num_experts)])\n",
        "\n",
        "\n",
        "        self.moe = MoE(self.expert, 100 * 32, 8)\n",
        "\n",
        "        self.output = nn.Linear(100, 260)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "\n",
        "        out = self.moe(x_emb)\n",
        "\n",
        "        logits = self.output(out)\n",
        "        return logits, None\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, block_size=32):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            idx_cond = idx[:, -block_size:] if idx.size(1) > block_size else idx\n",
        "\n",
        "\n",
        "            logits, _ = self(idx_cond)\n",
        "\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "8Xc_SxxMRMZd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, data, optimizer, criterion, device, epochs=3, batch_size=32, block_size=32):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i in range(0, len(data) - block_size - 1, batch_size):\n",
        "\n",
        "            indices = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "\n",
        "\n",
        "            x = torch.stack([data[idx:idx+block_size] for idx in indices])\n",
        "            y = torch.stack([data[idx+1:idx+block_size+1] for idx in indices])  # (B, block_size)\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / max(num_batches, 1)\n",
        "        print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "UQfwCaBKLkDe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()\n",
        "\n",
        "tokens = torch.tensor(tokenizer.encode(text[:10000], return_ids=True), dtype=torch.long)"
      ],
      "metadata": {
        "id": "JjibeyX2_LoE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Grok1()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(\n",
        "    model=model,\n",
        "    data=tokens,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device = 'cpu'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDxQfQ5RFnfy",
        "outputId": "202dd436-4206-43e1-f581-19fef024e4ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=3.0092\n",
            "Epoch 2: loss=1.5306\n",
            "Epoch 3: loss=0.9081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat(model, tokenizer, prompt, device, max_new_tokens):\n",
        "    model.eval()\n",
        "    input_ids = torch.tensor(tokenizer.encode(prompt, return_ids=True), dtype=torch.long, device=device)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0].tolist(), from_ids=True)\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "fRSLj7Bwcb4S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?:;'\\n-&3$#@э!?*\")\n",
        "\n",
        "vocab_size = 69\n",
        "\n",
        "prompt = \"We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely;\"\n",
        "\n",
        "response = chat(model, tokenizer, prompt, device='cpu', max_new_tokens=60)\n",
        "print('response:',response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWNiwIKRSQwk",
        "outputId": "977772a0-af75-45ab-fcab-1f595880bed8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response: We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely;E,,?,,gi''cc::::::::i uuuuuu iMeUUee tllllecuu:dddb:bdd:d:d\n"
          ]
        }
      ]
    }
  ]
}