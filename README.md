# Jumping-LLM-Flash
Jumping-LLM-Flash - библиотека для быстрой сборки маленьких языковых моделей, с фокусом на оптимизацию обучения, инференса, и работы с данными.
А также на уменьшение количества кода для построения полного пайплайна. 


Jumping-LLM-Flash is a library for quickly building small language models, focusing on optimizing training, inference, and data manipulation.
It also aims to reduce the amount of code needed to build a complete pipeline.

# Установка(Installation)
```bash
git https://github.com/companys1234/Jumping_LLM_Flash.git
```
# Тестовые примеры(Test cases)
LLaMa 2


<a href="https://colab.research.google.com/drive/1J0AtBk9unn7vXKNoF5-NixcIFuWXK5Pw?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
</a>
